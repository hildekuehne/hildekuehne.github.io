<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

	<head>
		<meta http-equiv="content-type" content="text/html;charset=utf-8" />
		<meta name="generator" content="Adobe GoLive" />
		<title>Hilde Kuehne - Computer Vision Group - Universtiy of Bonn</title>
		<link href="../../css/basic.css" rel="stylesheet" type="text/css" media="all" />
		<script>

		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),

		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)

		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-101254080-1', 'auto');
		  ga('send', 'pageview');

		</script>

	</head>

	<body>


<nav class="floating-menu">
<p>&nbsp;</p>
<a href="https://hildekuehne.github.io/">Home</a><br>
<a href="https://hildekuehne.github.io/index.html#projects">Projects</a><br>
<a href="https://hildekuehne.github.io/index.html#publications">Publications</a><br>
<a href="https://hildekuehne.github.io/index.html#cv">CV</a><br>
<br>
</nav>


		<table width="692" border="0" align="left" cellpadding="10" cellspacing="5" bgcolor="#FFFFFF">
			<tr>
				<td valign="top">
					<div align="center">
						<h1>Weakly supervised learning of actions from transcripts</h1>
						<p>H. Kuehne, A. Richard, J. Gall</p>
						<p></p>
						<br />
						<p><img src="Bild3_overview.png" alt="" height="320" border="0" /></p>
						<p><a href="https://arxiv.org/abs/1610.02237">.pdf</a> | <a href="./bib_entry.txt">.bibtext</a> </p>
						<p></p>
					</div>
					<h3>Abstract</h3>
					<p>We present an approach for weakly supervised learning of human actions from video transcriptions. Our system is based on the idea that, given a sequence of input data and a transcript, i.e. a list of the order the actions occur in the video, it is possible to infer the actions within the video stream, and thus, learn the related action models without the need for any frame-based annotation. Starting from the transcript information at hand, we split the given data sequences uniformly based on the number of expected actions. We then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts. The learned model can be used to temporally segment an unseen video with or without transcript. </p>
					<p></p>
					<h3>Results</h3>
					<h3><br />
					</h3>
					<div align="center">
						<p><img src="image_0200_3.png" alt="" height="240" border="0" /><br />
						</p>
						<p></p>
						<div align="center">
							<p><img src="s16-d06-cam-002_frame700_b.png" alt="" height="240" border="0" /></p>
						</div>
					</div>
					<h3></h3>
					<h3>Code</h3>
					<p>Will be available soon. If you want a notification, just contact kuehne [at] iai.uni-bonn.de .</p>
					<p></p>
				</td>
			</tr>
		</table>
	</body>

</html>