<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

	<head>
		<meta http-equiv="content-type" content="text/html;charset=utf-8" />
		<meta name="generator" content="Adobe GoLive" />
		<title>Hilde Kuehne - Computer Vision Group - Universtiy of Bonn</title>
		<link href="../../css/basic.css" rel="stylesheet" type="text/css" media="all" />
		<script>

		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),

		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)

		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-101254080-1', 'auto');
		  ga('send', 'pageview');

		</script>


	</head>

	<body>
<table border="0" width="692" cellspacing="5" cellpadding="10" align="left" bgcolor="#FFFFFF">
<tbody>
<tr>
<td valign="top">
<div align="center">
<p><img src="Webvision_header.jpg" alt="" height="320" border="0" /></p>
<h1><span style="font-size: x-large;">Welcome to the WebVision Video Track</span></h1>
<h3>&nbsp;</h3>
<h3>as part of the</h3>
<h3>&nbsp;</h3>
<h3>THE 4TH WORKSHOP ON</h3>
<h3>VISUAL UNDERSTANDING BY LEARNING FROM WEB DATA 2020</h3>
<h3>June 14th - 19th, 2020</h3>
<h3>Seattle, WA</h3>
<h3>in conjunction with&nbsp;<a href="http://cvpr2020.thecvf.com/">CVPR 2020</a></h3>
<p style="text-align: justify;">&nbsp;</p>
<p style="text-align: justify;">Collecting data for large-scale action classification is becomming more and more time consuming. This puts a natural limit to the size of current benchmarks and makes it unlikely to ever have ImageNet scale benchmarks in action recognition with millions of samples. Additionally, those datasets are usually based on a hand-crafted class vocabulary based on easy to search categories as authors need to cover many different scenarios and at the same time, identify unique distinguishable actions. But techniques developed and finetuned on such data do not naturally transfer to applications in the wild. To adress this problem, we want to move some steps away from the usual action classification&nbsp;and explore the problem of learning actions from real-live videos without human supervision.&nbsp;</p>
<p style="text-align: left;">The webvision video track run as part of the Workshop on Visual Understanding by Learning from Web Data. This workshop aims at promoting the advance of learning state-of-the-art visual models from webly supervised data. We want to transfer this idea to the case of learning action representations from video subtitles without any human supervision.</p>
<h3 style="text-align: left;">Two tracks</h3>
<p style="text-align: left;">We will have two tracks for this challenge, one based on the original videos and one based on precomputed features only.</p>
<p style="text-align: left;"><strong>Video Track:</strong></p>
<p style="text-align: left;">For the orginal video track, you are free to use the full videos or the pre-extracted video clips.&nbsp;</p>
<p style="text-align: left;"><strong>Feature Track:</strong></p>
<p style="text-align: left;">For the feature track, you are only allowed to use the preextracted features from the 350k clips.&nbsp;</p>
<p style="text-align: left;">We will provide a vanilla pytorch baseline for the feature track as well as evalaution scripts to reproduce intial results and to valiadate your systems.&nbsp;</p>
<h3 style="text-align: justify;">&nbsp;Data</h3>
<p style="text-align: justify;">The data for this challenge is based on the MiningYouTube dataset (working title is 'Weak YouTube', but some people found that this sounds too negative). Please find the homepage here: <a href="https://github.com/hildekuehne/Weak_YouTube_dataset" target="_blank" rel="noopener">https://github.com/hildekuehne/Weak_YouTube_dataset</a>&nbsp;. The dataset comprises ~ 20,000 YouTube videos that display explain various egg recipes, namely for fried egg, scrambled egg, pancake, omelet, and egg roll.</p>
<p style="text-align: justify;">For training, we provide the video indexes with the respective subtitles (downloaded in 2017/2018) as well as pre-extracted video clips with tentative labels as described in the paper. For the feature-track, we further provide pre-computed TSN features (<a href="https://github.com/yjxiong/temporal-segment-networks" target="_blank" rel="noopener">https://github.com/yjxiong/temporal-segment-networks</a>) based on the Kinetics pretrained model. We hope that working with pre-computed features will allow a faster development and testing especially of new ideas on the mining and/or concept learning side and at the same time allow for an easier reproducability of methods. The training data for all tracks is available under: <a href="https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/train" target="_blank" rel="noopener">https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/train</a></p>
<p style="text-align: justify;">The data additionally has a set of ~5000 videos with class labels and a human annotation if this class label is present in the video, which can be used for training or validation.</p>
<p style="text-align: justify;">The testing data (and validation data for the challenge) is available under: <a href="https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/test" target="_blank" rel="noopener">https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/test</a></p>
<p style="text-align: justify;">&nbsp;</p>
<p style="text-align: justify;"><strong>General rules:</strong></p>
<p style="text-align: justify;">You are allowed to use the yes/no validation data listed in the 'val_yes_no.txt' file (here: <a href="https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/train">https://github.com/hildekuehne/Weak_YouTube_dataset/tree/master/train</a>) for validation and/or training. It's only a few clips per class, so the assumption is that it will not get you all the way, but any new ideas are welcome.</p>
<p style="text-align: justify;">You are only allowed to use the orginal subititles or the generated labels from the baseline. Please do not! download new subtitles as they can change over time and we would not be able to compare your methods to others any more.</p>
<p style="text-align: justify;">You can use the test set of the original dataset as validation set. It is not allowed to include the data from the test set as additional training data!</p>
<p style="text-align: justify;">&nbsp;</p>
<h3 style="text-align: left;">Frequently Asked Questions</h3>
<p style="text-align: justify;"><br /><strong>Can I use pretrained models for action classification?</strong><br />Yes, but! the model or the data that you use have to be freely available for everyone (e.g. the TSN or I3D models). You can also pretrain your own model by using a single standard datasets like Kinetics, ActivityNet etc. You need to indicate what dataset was used for pretraining or which backbone you used. You are not allowed to mix various datasets for better pretraining. You are not allowed to use any datasets or models for pretraining that are not publically available.</p>
<p style="text-align: justify;"><br /><strong>Can I use other/more videos without human annotations?</strong><br />No. For fairness, we restrict the challenge to use only the videos provided with the MiningYouTube datasets. You are allowed to use one (!) more public action dataset for pretraining. You are not allowed to crawl for more videos by yourself.</p>
<p style="text-align: justify;"><br /><strong>Can I use the text data (subtitles) in the MiningYouTube dataset?</strong><br />Yes, and we encourage you to do so. You are not allowed to download updated subtitles for the videos!</p>
<p style="text-align: justify;"><br /><strong>Can I use external text data, or models pretrained with external text data, with or without human annotation?</strong><br />Yes, but any data or model has to be publicly available, e.g. WordNet, Knowledge Graph, etc. can be used. Models trained using external text data are also allowed, such as Word2Vec, BERT models, as the data is available or made available. You need to explicitly state in your final submission that what text datasets/models are used.</p>
<p style="text-align: justify;"><strong>Can I crawl text data according to MiningYouTube concepts by myself, and use it as training data?</strong><br />Yes, as long as the data is publicly and legally available, so people could reproduce the results. If you crawl text data by yourself, please clearly state it in your submission, and you need to make it available to public before the final submission deadline. An URL should be provided in the method description part of your submission.</p>
<p style="text-align: justify;"><br />If you have other questions, please drop an email to webvisionworkshop AT gmail.com</p>
</div>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
	</body>

</html>
