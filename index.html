<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	
	<title>Hilde Kuehne - Personal Homepage</title>

	<!-- <link rel="shortcut icon" href="assets/images/gt_favicon.png">  -->
	
	<!-- Bootstrap -->
	<link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css">
	<!-- Icons -->
	<link rel="stylesheet" href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"/>
	<!-- Fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
    <!-- Custom styles -->
	<link rel="stylesheet" href="assets/css/styles.css">

	<!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
    
		<script>

		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),

		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)

		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-101254080-1', 'auto');
		  ga('send', 'pageview');

		</script>
    
</head>
<body class="home">

<header id="header">
	<div id="head" class="parallax" parallax-speed="2">
		<h1 id="logo" class="text-center">
			<img class="img-circle" src="img/Hilde.jpg" alt="">
			<span class="title">Hilde Kuehne</span>
			<span class="tagline">
                <a href="https://cg.cs.uni-bonn.de/">University of Bonn</a><br>
                <a href="https://mitibmwatsonailab.mit.edu/people/hildegarde-kuhne/">MIT-IBM Watson AI Lab</a><br>
				<a href="mailto:hilde.kuehne@uni-bonn.de">hilde.kuehne@uni-bonn.de</a><br>
				<a href="mailto:kuehne@uni-frankfurt.de">old: kuehne@uni-frankfurt.de</a></span>
                <p>
                    <a href="https://scholar.google.com/citations?user=pxhCcH0AAAAJ/"><i class="ai ai-google-scholar-square ai-2"></i></a>
                    <a href="https://twitter.com/HildeKuehne"><i class="fa fa-twitter fa-2"></i></a>
                    <a href="https://www.linkedin.com/in/hilde-kuehne-8b9aa661"><i class="fa fa-linkedin fa-2"></i></a>
                </p>
		</h1>
	</div>
<!--
	<nav class="navbar navbar-default navbar-sticky">
		<div class="container-fluid">
			
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
			</div>
			
			<div class="navbar-collapse collapse">
				
				<ul class="nav navbar-nav">
					<li class="active"><a href="index.html">Home</a></li>
					<li><a href="about.html">About</a></li>
					<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown">More Pages <b class="caret"></b></a>
						<ul class="dropdown-menu">
							<li><a href="sidebar-left.html">Left Sidebar</a></li>
							<li><a href="sidebar-right.html">Right Sidebar</a></li>
							<li><a href="single.html">Blog Post</a></li>
						</ul>
					</li>
					<li><a href="blog.html">Blog</a></li>
				</ul>
			
			</div>	
		</div>	
	</nav>  
-->	
</header>

<main id="main">

	<div class="container">
		 <!--
		<div class="row section topspace">
			<div class="col-md-12">
				<p class="lead text-center text-muted">Let me tell you something my friend. hope is a dangerous thing. hope can drive a man insane. you <a href="about.html">measure</a> yourself by the people who measure themselves by you. it only took me <a href="sidebar-right.html">six days</a>. </p>
			</div>
		</div> 
        ---- / section -->
		
		<div class="row section clients topspace">
			<!-- <h3 class="section-title" align="right" ><span>News</span></h3>  -->
			<div class="col-lg-12">
						<p align="right" style="text-align: left;">
                            <h3 align="left" >News</h3> 
							<li type="disc">I'm starting a new lab at the University of Bonn. Stay tuned for details! 
				                        <li type="disc">I'm Progamm Chair for <a href="https://wacv2024.thecvf.com/">WACV 2024</a>
							<li type="disc">Two papers accteped for CVPR 2023 - Big congrats to Aisha and Wei and everybody else involved! </li>
							<li type="disc">Three papers accteped for ICLR 2023 - Big congrats to Anna, Felix, and Yuan and everybody else involved! </li>
							<li type="disc">The website of our MIT-IBM "Sight and Sound" project is finally online! Check out all the great papers and people working on this: <a href="http://sightandsound.csail.mit.edu/">http://sightandsound.csail.mit.edu/</a> </li>
                            <!--
						    <li type="disc">Our paper "Differentiable Top-k Classification Learning" has been accteped for ICML 2022 - Big congrats to Felix - Details follow soon!</li>
							<li type="disc">Our Paper "Learning with Algorithmic Supervision via Continuous Relaxations" is presented at NeurIPS 2021 </li>
							<li type="disc">HMDB was awarded this years TC-PAMI Helmholtz Award at ICCV2021! </li>
							<li type="disc">I'm giving a talk about at ICCV 2021 "Second International Tutorial on Large Scale Holistic Video Understanding" on Understanding Videos Without Labels - <a href="https://www.youtube.com/watch?v=KwVdGALzJo8">YouTube</a> </li>
							<li type="disc">I'm giving a talk about at ICCV 2021 "6th Workshop on Benchmarking Multi-Target Tracking" on Spatial-temporal action localization in untrimmed videos </li>
							<li type="disc">I'm serving as area chair for CVPR 2021 and 2022, ICCV 2021, BMVC 2021, and  WACV 2021 and 2022</li>
                            ---- old news -->
						</p>
			</div>
		</div> <!-- /section -->

		<div class="row section featured topspace">
			<h2 class="section-title"><span>Papers</span></h2>

            <table cellspacing="5" cellpadding="5" border="0" >
            <tbody>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/LbS_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Learning by Sorting: Self-supervised Learning with Group Ordering Constraints</b> <br />
                            Nina Shvetsova, Felix Petersen, Anna Kukleva, Bernt Schiele, Hilde Kuehne <br /> 
                            arxiv 2023 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://arxiv.org/abs/2301.02009">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/SHG_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Learning Situation Hyper-Graphs for Video Question Answering</b> <br />
                            Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang Gan, Niels Lobo, Mubarak Shah <br /> 
                            CVPR 2023 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.html">pdf</a>), 
                            (<a href="https://github.com/aurooj/SHG-VQA">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/VTTA_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Video Test-Time Adaptation for Action Recognition</b> <br />
                            Wei Lin, Muhammad Jehanzeb Mirza, Mateusz Kozinski, Horst Possegger, Hilde Kuehne, Horst Bischof <br /> 
                            CVPR 2023 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Video_Test-Time_Adaptation_for_Action_Recognition_CVPR_2023_paper.html">pdf</a>), 
                            (<a href="https://github.com/wlin-at/ViTTA">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/TempS_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Temperature Schedules for self-supervised contrastive methods on long-tail data</b> <br />
                            Anna Kukleva, Moritz Boehle, Bernt Schiele, Hilde Kuehne, Christian Rupprecht<br /> 
                            arxiv 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openreview.net/forum?id=ejHUr4nfHhD">pdf</a>), 
                            (<a href="https://github.com/annusha/temperature_schedules">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/ISAAC_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>ISAAC Newton: Input-based Approximate Curvature for Newton's Method</b> <br />
                            Felix Petersen, Tobias Sutter, Christian Borgelt, Dongsung Huh, Hilde Kuehne, Yuekai Sun, Oliver Deussen<br /> 
                            ICLR 2023 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openreview.net/forum?id=0paCJSFW7j">pdf</a>), (<a href="https://github.com/felix-petersen/isaac">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/CAVMAE_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Contrastive audio-visual masked autoencoder</b> <br />
                            Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James Glass <br /> 
                            ICLR 2023</div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openreview.net/forum?id=QPtMRyk5rb">pdf</a>), (<a href="https://github.com/YuanGongND/cav-mae">code</a>)  <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/LogicGate_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Deep Differentiable Logic Gate Networks</b> <br />
                            Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen <br /> 
                            NeurIPS 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://openreview.net/forum?id=vF3WefcoePW">pdf</a>), (<a href="https://github.com/Felix-Petersen/difflogic">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/C2KD_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</b> <br />
                            Andrew Rouditchenko, Yung-Sung Chuang, Nina Shvetsova, Samuel Thomas, Rogerio Feris, Brian Kingsbury, Leonid Karlinsky, David Harwath, Hilde Kuehne, James Glass <br /> 
                            arxiv 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://arxiv.org/abs/2210.03625">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/VLTaboo_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>VL-Taboo: An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models</b> <br />
                            Felix Vogel, Nina Shvetsova, Leonid Karlinsky, Hilde Kuehne <br /> 
                            arxiv 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://arxiv.org/abs/2209.06103">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/diffTopK_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Differentiable top-k classification learning</b> <br />
                            Felix Petersen, Hilde Kuehne, Christian Borgelt, Oliver Deussen <br /> 
                            ICML 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://proceedings.mlr.press/v162/petersen22a.html">pdf</a>), 
                            (<a href="https://github.com/Felix-Petersen/difftopk">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/AugL_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Augmentation Learning for Semi-Supervised Classification</b> <br />
                            Tim Frommknecht, Pedro Alves Zipf, Quanfu Fan, Nina Shvetsova, Hilde Kuehne <br /> 
                            GCPR 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="http://128.84.21.203/abs/2208.01956">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/CycleDA_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>CycDA: Unsupervised Cycle Domain Adaptation to Learn from Image to Video</b> <br />
                            Wei Lin, Anna Kukleva, Kunyang Sun, Horst Possegger, Hilde Kuehne, Horst Bischof  <br /> 
                            ECCV 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1118_ECCV_2022_paper.php">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/VQAT_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Weakly Supervised Grounding for VQA in Vision-Language Transformers</b> <br />
                            Aisha Urooj Khan, Hilde Kuehne, Chuang Gan, Niels Da Vitoria Lobo, Mubarak Shah <br /> 
                            ECCV 2022 (Oral) </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1011_ECCV_2022_paper.php">pdf</a>), 
                            (<a href="https://github.com/aurooj/wsg-vqa-vltransformers">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/EAO_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 430.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval</b> <br />
                            Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel Thomas, Brian Kingsbury, Rogerio Feris, David Harwath, James Glass, Hilde Kuehne. <br /> 
                            CVPR 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />
                            (<a href="https://arxiv.org/abs/2112.04446">pdf</a>), 
                            (<a href="https://github.com/ninatu/everything_at_once">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
	  			<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/BRAD_teaser.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
                            <b>Unsupervised Domain Generalization by Learning a Bridge Across Domains</b> <br />
							Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, Dina Katabi, Kate Saenko, Rogerio Feris, Leonid Karlinsky.  <br />
                            CVPR 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2112.02300">pdf</a>), 
                            (<a href="https://github.com/leokarlin/BrAD">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/monotonic_sorting.jpg" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
                            <b>Monotonic Differentiable Sorting Networks </b> <br />
                            Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen. <br />
                            ICLR 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://openreview.net/forum?id=IcUWShptD7d">pdf</a>), (<a href="https://github.com/Felix-Petersen/diffsort">Code</a>), (<a href="https://www.youtube.com/watch?v=Rl-sFaE1z4M">YouTube</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/LZ0Z1A.gif" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
                            <b>Style Agnostic 3D Reconstruction via Adversarial Style Transfer</b> <br />
							Felix Petersen, Bastian Goldluecke, Oliver Deussen, Hilde Kuehne.  <br />
                            WACV 2022 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://openaccess.thecvf.com/content/WACV2022/html/Petersen_Style_Agnostic_3D_Reconstruction_via_Adversarial_Style_Transfer_WACV_2022_paper.html">pdf</a>), (<a href="https://github.com/Felix-Petersen/style-agnostic-3d-reconstruction">Code</a>), (<a href="https://www.youtube.com/watch?v=swDYD3v74i0">YouTube</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/algovision.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
                            <b>Learning with Algorithmic Supervision via Continuous Relaxations</b> <br />
                            Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen.  <br />
                            NeurIPS 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://papers.nips.cc/paper/2021/hash/89ae0fe22c47d374bc9350ef99e01685-Abstract.html">pdf</a>), (<a href="https://github.com/felix-petersen/algovision">code</a>), (<a href="https://www.youtube.com/watch?v=01ENzpkjOCE">Youtube</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/grounding_leonid.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Detector-Free Weakly Supervised Grounding by Separation</b> <br /> 
                            Assaf Arbelle, Sivan Doveh, Amit Alfassy, Joseph Shtok, Guy Lev, Eli Schwartz, Hilde Kuehne, Hila Barak Levi, Prasanna Sattigeri, Rameswar Panda, Chun-Fu Chen, Alex Bronstein, Kate Saenko, Shimon Ullman, Raja Giryes, Rogerio Feris, Leonid Karlinsky.  <br />
                            ICCV 2021 (oral) </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/pdf/2104.09829">pdf</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/MCN.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos</b> <br /> 
                            Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel Thomas, Angie Boggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris, David Harwath, James Glass, Michael Picheny, Shih-Fu Chang.  <br />
                            ICCV 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2104.12671">pdf</a>), (<a href="https://github.com/brian7685/Multimodal-Clustering-Network">code</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/g_fewshot.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting</b> <br /> 
                            Anna Kukleva, Hilde Kuehne, Bernt Schiele.  <br />
                            ICCV 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br /> (<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Kukleva_Generalized_and_Incremental_Few-Shot_Learning_by_Explicit_Learning_and_Calibration_ICCV_2021_paper.html">pdf</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/avlnet.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>AVLnet: Learning Audio-Visual Language Representations from Instructional Videos</b> <br />
                            Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, James Glass.   <br />
                            Interspeech 2021</div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2006.09199">pdf</a>), (<a href="https://github.com/roudimit/AVLnet">AVLNet code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/cascaded.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Cascaded Multilingual Audio-Visual Learning from Videos</b> <br />  
                            Andrew Rouditchenko, Angie Boggust, David Harwath, Samuel Thomas, Hilde Kuehne, Brian Chen, Rameswar Panda, Rogerio Feris, Brian Kingsbury, Michael Picheny, James Glass.  <br />
                            Interspeech 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/rouditchenko21b_interspeech.pdf">pdf</a>), (<a href="https://github.com/roudimit/AVLnet">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/sorting.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision</b> <br />
                            Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen.  <br />
                            ICML 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2105.04019">pdf</a>), (<a href="https://github.com/Felix-Petersen/diffsort">DiffSort code</a>), (<a href="https://www.youtube.com/watch?v=38dvqdYEs1o">YouTube</a>) <br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/gqa_aisha.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules</b> <br /> 
                            Aisha Urooj Khan, Hilde Kuehne, Kevin Duarte, Chuang Gan, Niels Lobo, Mubarak Shah.  <br />
                            CVPR 2021</div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2105.04836">pdf</a>), (<a href="https://github.com/aurooj/WeakGroundedVQA_Capsules">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/Latent.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
                            <b>Unsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities</b> <br /> 
                            Sirnam Swetha, Hilde Kuehne, Yogesh S Rawat, Mubarak Shah.  <br /> 
                            ICIP 2021 </div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/2105.00067">pdf</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/rvidal_icon.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Joint visual-temporal embedding for unsupervised learning of actions in untrimmed sequences</b> <br /> 
                            Rosaura G VidalMata, Walter J Scheirer, Anna Kukleva, David Cox, Hilde Kuehne.  <br />
                            WACV 2021</div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://openaccess.thecvf.com/content/WACV2021/papers/VidalMata_Joint_Visual-Temporal_Embedding_for_Unsupervised_Learning_of_Actions_in_Untrimmed_WACV_2021_paper.pdf">pdf</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/exp_lb_thumb.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation</b> <br /> 
                            Quanfu Fan, Chun-Fu (Richard) Chen, Hilde Kuehne, Marco Pistoia, David Cox.  <br />
                            NeurIPS 2019</div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://papers.nips.cc/paper/8498-more-is-less-learning-efficient-video-representations-by-big-little-network-and-depthwise-temporal-aggregation">pdf</a>), (<a href="https://github.com/IBM/bLVNet-TAM">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
 				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/exp_unsupervised_thumb.jpg" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Unsupervised learning of action classes with continuous temporal embedding</b> <br /> 
                            A. Kukleva*, H. Kuehne*, F. Sener, J. Gall.  <br />
                            CVPR 2019</div>
						<p style="margin: 0cm 0cm 0pt;"><br />(<a href="https://arxiv.org/abs/1904.04189">pdf</a>), (<a href="https://github.com/Annusha/unsup_temp_embed">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/richard_hybrid_rnn_hmm.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation</b> <br />
                            Hilde Kuehne*, Alexander Richard*, Juergen Gall. <br />
                            IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 2019 (open access)<br />
						</div>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="https://ieeexplore.ieee.org/document/8585084">pdf</a>)<br />
							<br />
						</p>
					</td>
				</tr>
 				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/NNViterbi_thumb.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning</b> <br /> 
                            Alexander Richard, Hilde Kuehne, Ahsan Iqbal, Juergen Gall.  <br />
                            CVPR 2018</div>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="https://alexanderrichard.github.io/publications/pdf/richard_nn_viterbi.pdf">pdf</a>), (<a href="https://github.com/alexanderrichard/NeuralNetwork-Viterbi">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/Actionsets_thumb.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Action Sets: Weakly Supervised Action Segmentation without Ordering Constraints</b> <br /> 
                            Alexander Richard, Hilde Kuehne, Juergen Gall.  <br />
                            CVPR 2018</div>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="https://alexanderrichard.github.io/publications/pdf/richard_action_sets.pdf">pdf</a>), (<a href="https://alexanderrichard.github.io/publications/bibtex/richard_action_sets.bib">bibtex</a>), (<a href="https://github.com/alexanderrichard/action-sets">code</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/RNN_ashan_thumb.png" alt="" width="150" border="0"  /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<div>
							<b>Recurrent Residual Learning for Action Recognition, German Conference on Pattern Recognition</b> <br />
                            Ahsan Iqbal, Alexander Richard, Hilde Kuehne, Juergen Gall.   <br />
                            GCPR 2017 (Best Master's Award)</div>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="https://arxiv.org/abs/1706.08807">pdf</a>), (<a href="http://dblp.uni-trier.de/rec/bibtex/conf/dagm/IqbalRKG17">bibtex</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/RNN_thumb.png" alt="" width="150" border="0" /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<p style="margin: 0cm 0cm 0pt;">
                            <b>Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling</b> <br /> 
                            A. Richard, H. Kuehne and J. Gall.  <br />
                            CVPR 2017 (oral)</p>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="projects/rnnActionModeling/index.html">website &amp; downloads</a>)<br />
							<br />
						</p>
					</td>
				</tr>
				<tr>
					<td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
						<p align="right" style="text-align: right;"><img src="img/Weak_thumb.png" alt="" width="150" border="0" /><em><strong><br />
								</strong></em></p>
					</td>
					<td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
						<p> </p>
					</td>
					<td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
						<p style="margin: 0cm 0cm 0pt;"></p>
						<p style="margin: 0cm 0cm 0pt;">
                            <b>Weakly supervised learning of actions from transcripts</b> <br />
                            H. Kuehne, A. Richard and J. Gall.   <br />
                            CVIU 2017</p>
						<p style="margin: 0cm 0cm 0pt;"><br />
							(<a href="projects/weakLearning/index.html">website &amp; downloads</a>)<br />
							<br />
						</p>
					</td>
				</tr>
	<tr>
      <td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><img src="img/WACV_thumb.gif" alt="" width="150" border="0" /><em><strong><br />
								</strong></em></p>
        </td>
      <td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;"> <p style="margin: 0cm 0cm 0pt;"></p>
						<p style="margin: 0cm 0cm 0pt;">
                            <b>An end-to-end generative framework for video segmentation and recognition</b> <br /> 
                            H. Kuehne, J. Gall and T. Serre.  <br />
                            WACV 2016</p>
						<p style="margin: 0cm 0cm 0pt;"> <br />
          (<a href="projects/end2end/index.html">website
          & downloads</a>)<br>
          <br />
        </p></td>
    </tr>
				<tr>
      <td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><img src="img/CVPR14_thumb.png" alt="" width="150" border="0" /><em><strong><br />
								</strong></em></p>
        </td>
      <td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;"> <p style="margin: 0cm 0cm 0pt;"></p>
						<p style="margin: 0cm 0cm 0pt;">
                            <b>The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities</b> <br />
                            H. Kuehne, A. B. Arslan and T. Serre.   <br />
                            CVPR 2014</p>
						<p style="margin: 0cm 0cm 0pt;"> <br />
							<a href="http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/">(Breakfast dataset: data &amp; code)</a><br>
							<br />
        </p></td>
    </tr>
				<tr>
      <td valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><img src="img/Visapp12_thumb.png" alt="" width="150" border="0" /></p></td>
      <td valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;"> <p style="margin: 0cm 0cm 0pt;"></p>
						<p style="margin: 0cm 0cm 0pt;">
                            <b>On-line Action Recognition from sparse Feature Flow</b> <br /> 
                            H. Kuehne, D. Gehrig, T. Schultz, R. Stiefelhagen.  <br />
                            VISAPP 2012</p>
						<p style="margin: 0cm 0cm 0pt;"><br />
						</p>
						<p style="margin: 0cm 0cm 0pt;">(<a href="http://www.sfb588.uni-karlsruhe.de/about/downloads_bktdata.html">data &amp; annotations</a>)<br>
							<br />
						</p>
					</td>
    </tr>
				<tr>
      <td width="151" valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><img src="img/HMDB_thumb.png" alt="" width="150" border="0" /></p></td>
      <td width="16" valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td width="440" valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
        <p style="margin: 0cm 0cm 0pt;">
            <b>HMDB: A Large Video Database for Human Motion Recognition</b> <br />
            H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, T. Serre.  <br />
            ICCV 2011</p>
        <p style="margin: 0cm 0cm 0pt;"> <br />
          (<a href="http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/" target="_blank">project
          website</a>) <br>
          <br />
        </p></td>
    </tr>
				<tr>
      <td width="151" valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><strong><em><span lang="EN-US" style="">Visapp
          2010<br />
          </span></em></strong><strong><em><span lang="EN-US" style="">Angers,
          France</span></em></strong></p>
        <p align="right" style="text-align: right;">&nbsp;</p></td>
      <td width="16" valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td width="440" valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
        <p><span lang="EN-US" style="">
          <b>Motion Segmentation of Articulated Structures by Integration of Visula Perception Criteria</b> <br />
          H. Kuehne, A. Woerner.  <br />
          VisApp 2010</span><br />
          <span lang="EN-US" style=""> <br />
          (<a href="https://pdfs.semanticscholar.org/260a/74d1cb594dc49b3e4c154f4b7558c7321e1e.pdf" target="_blank">pdf</a>)(<a href="http://dblp.uni-trier.de/rec/bibtex/conf/visapp/KuehneW10" target="_blank">bibtex</a>)<br />
          <br />
          </span></p></td>
    </tr>
				<tr>
      <td width="151" valign="top" style="border-style: none solid none none; border-color: -moz-use-text-color windowtext -moz-use-text-color -moz-use-text-color; border-width: medium 1pt medium medium; padding: 0cm 5.4pt; width: 4cm;">
        <p align="right" style="text-align: right;"><strong><em><span lang="EN-US" style="">ICCV&nbsp;2009,
          <br />
          </span></em></strong><strong><em><span lang="EN-US" style="">&nbsp;Kyoto,
          Japan</span></em></strong></p></td>
      <td width="16" valign="top" style="border: medium none ; padding: 0cm 5.4pt; width: 11.8pt;">
        <p>&nbsp;</p></td>
      <td width="440" valign="top" style="padding: 0cm 5.4pt; width: 330.2pt;">
        <p style="margin: 0cm 0cm 0pt;"><span lang="EN-US" style="line-height: 115%;">
            <b>An Iterative Scheme for Motion-Based Scene Segmentation</b> <br />
            A.Bachmann, H. Kuehne.  <br />
            ICCV 2009, Workshop on Dynamical Vision (DV)<br />
          <br />
          (<a href="http://digbib.ubka.uni-karlsruhe.de/volltexte/documents/1624622" target="_blank">pdf</a>)(<a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5457631" target="_blank">bibtex</a>)</span></p>
        <p style="margin: 0cm 0cm 0pt;">&nbsp;</p></td>
    </tr>
	</tbody>
    </table>

        
        </div> <!-- / section -->
	
		<div class="row section recentworks topspace">
		</div> <!-- /section -->
 

	</div>	<!-- /container -->

</main>

<footer id="footer">
	<div class="container">
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">
			
			<div class="col-md-6 widget">
				<div class="widget-body">
					<p></p>
				</div>
			</div>

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2023<br> 
						Design: <a href="http://www.gettemplate.com" rel="designer">Initio by GetTemplate</a> </p>
				</div>
			</div>

		</div> <!-- /row of widgets -->
	</div>
</footer>



<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="http://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
</body>
</html>
